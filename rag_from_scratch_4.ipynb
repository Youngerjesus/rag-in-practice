{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rag From Scratch: Indexing\n",
    "\n",
    "![](./images/Indexing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in ./.venv/lib/python3.12/site-packages (0.2.5)\n",
      "Requirement already satisfied: tiktoken in ./.venv/lib/python3.12/site-packages (0.7.0)\n",
      "Requirement already satisfied: langchain-openai in ./.venv/lib/python3.12/site-packages (0.1.9)\n",
      "Requirement already satisfied: langchainhub in ./.venv/lib/python3.12/site-packages (0.1.20)\n",
      "Requirement already satisfied: chromadb in ./.venv/lib/python3.12/site-packages (0.5.3)\n",
      "Requirement already satisfied: langchain in ./.venv/lib/python3.12/site-packages (0.2.5)\n",
      "Requirement already satisfied: youtube-transcript-api in ./.venv/lib/python3.12/site-packages (0.6.2)\n",
      "Requirement already satisfied: pytube in ./.venv/lib/python3.12/site-packages (15.0.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.12/site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.12/site-packages (from langchain_community) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.12/site-packages (from langchain_community) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.venv/lib/python3.12/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.7 in ./.venv/lib/python3.12/site-packages (from langchain_community) (0.2.9)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in ./.venv/lib/python3.12/site-packages (from langchain_community) (0.1.81)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in ./.venv/lib/python3.12/site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.12/site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.venv/lib/python3.12/site-packages (from langchain_community) (8.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.12/site-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.26.0 in ./.venv/lib/python3.12/site-packages (from langchain-openai) (1.35.3)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.12/site-packages (from langchainhub) (24.1)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in ./.venv/lib/python3.12/site-packages (from langchainhub) (2.32.0.20240622)\n",
      "Requirement already satisfied: build>=1.0.3 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.2.1)\n",
      "Requirement already satisfied: pydantic>=1.9 in ./.venv/lib/python3.12/site-packages (from chromadb) (2.7.4)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in ./.venv/lib/python3.12/site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in ./.venv/lib/python3.12/site-packages (from chromadb) (0.111.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.30.1)\n",
      "Requirement already satisfied: posthog>=2.4.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.18.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in ./.venv/lib/python3.12/site-packages (from chromadb) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.25.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./.venv/lib/python3.12/site-packages (from chromadb) (0.19.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./.venv/lib/python3.12/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (4.66.4)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./.venv/lib/python3.12/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./.venv/lib/python3.12/site-packages (from chromadb) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (1.64.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./.venv/lib/python3.12/site-packages (from chromadb) (4.1.3)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (0.12.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (30.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./.venv/lib/python3.12/site-packages (from chromadb) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./.venv/lib/python3.12/site-packages (from chromadb) (3.10.5)\n",
      "Requirement already satisfied: httpx>=0.27.0 in ./.venv/lib/python3.12/site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.venv/lib/python3.12/site-packages (from langchain) (0.2.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
      "Requirement already satisfied: pyproject_hooks in ./.venv/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in ./.venv/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in ./.venv/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb) (0.0.4)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in ./.venv/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb) (3.1.4)\n",
      "Requirement already satisfied: python-multipart>=0.0.7 in ./.venv/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb) (0.0.9)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in ./.venv/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb) (5.10.0)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in ./.venv/lib/python3.12/site-packages (from fastapi>=0.95.2->chromadb) (2.2.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (4.4.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.0.5)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.30.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in ./.venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.7->langchain_community) (1.33)\n",
      "Requirement already satisfied: coloredlogs in ./.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.26.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in ./.venv/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=7.1,>=6.0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.1.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in ./.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.25.0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.25.0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.25.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.46b0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.46b0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.46b0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.46b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (70.1.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in ./.venv/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./.venv/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in ./.venv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./.venv/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb) (0.23.4)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in ./.venv/lib/python3.12/site-packages (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb) (2.6.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.venv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.venv/lib/python3.12/site-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.venv/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in ./.venv/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain youtube-transcript-api pytube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 12: Multi-representation Indexing\n",
    "\n",
    "Docs:\n",
    "- https://blog.langchain.dev/semi-structured-multi-modal-rag/\n",
    "- https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector\n",
    "\n",
    "Paper:\n",
    "- https://arxiv.org/abs/2312.06648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | ChatOpenAI(model=\"gpt-3.5-turbo\",max_retries=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\",\n",
    "                     embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# Docs linked to summaries\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# Add\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='The document discusses the concept of building autonomous agents powered by large language models (LLMs). It covers key components of such agents, including planning, memory, and tool use, along with case studies and proof-of-concept examples. Challenges such as limited context length, planning difficulties, and reliability of natural language interfaces are also highlighted. The document provides insights into the potential of LLMs for creating powerful general problem solvers and showcases various examples of LLM-powered autonomous agents.', metadata={'doc_id': 'e01c3d93-a0d6-4ec3-adc2-bdd109d646d4'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Memory in agents\"\n",
    "sub_docs = vectorstore.similarity_search(query,k=1)\n",
    "sub_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongmin/PycharmProjects/rag-in-practice/.venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(query,n_results=1)\n",
    "retrieved_docs[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 13: RAPTOR\n",
    "\n",
    "Deep dive video:\n",
    "- https://www.youtube.com/watch?v=jbGchdTL7d0\n",
    "\n",
    "Paper:\n",
    "- https://arxiv.org/pdf/2401.18059.pdf\n",
    "\n",
    "Full code:\n",
    "- https://github.com/langchain-ai/langchain/blob/master/cookbook/RAPTOR.ipynb\n",
    "\n",
    "\n",
    "![](./images/RAPTOR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RAPTOR paper presents an interesting approaching for indexing and retrieval of documents:\n",
    "- The leafs are a set of starting documents\n",
    "- Leafs are embedded and clustered\n",
    "- Clusters are then summarized into higher level (more abstract) consolidations of information across similar documents\n",
    "\n",
    "This process is done recursivly, resulting in a \"tree\" going from raw docs (leafs) to more abstract summaries.\n",
    "\n",
    "We can applying this at varying scales; leafs can be:\n",
    "- Text chunks from a single doc (as shown in the paper)\n",
    "- Full docs (as we show below)\n",
    "With longer context LLMs, it's possible to perform this over full documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (3.9.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: umap in ./.venv/lib/python3.12/site-packages (0.1.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading scikit_learn-1.5.0-cp312-cp312-macosx_12_0_arm64.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.0-cp312-cp312-macosx_14_0_arm64.whl (23.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.0 scipy-1.14.0 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib pandas umap scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "# LCEL docs\n",
    "url = \"https://python.langchain.com/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# LCEL w/ PydanticOutputParser (outside the primary LCEL docs)\n",
    "url = \"https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_pydantic = loader.load()\n",
    "\n",
    "# LCEL w/ Self Query (outside the primary LCEL docs)\n",
    "url = \"https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_sq = loader.load()\n",
    "\n",
    "# Doc texts\n",
    "docs.extend([*docs_pydantic, *docs_sq])\n",
    "docs_texts = [d.page_content for d in docs]\n",
    "\n",
    "# Calculate the number of tokens for each document\n",
    "counts = [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n",
    "\n",
    "# Plotting the histogram of token counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Histogram of Token Counts\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "\n",
    "# Display the histogram\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens in all context: 5060\n"
     ]
    }
   ],
   "source": [
    "# Doc texts concat\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")\n",
    "print(\n",
    "    \"Num tokens in all context: %s\"\n",
    "    % num_tokens_from_string(concatenated_content, \"cl100k_base\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc texts split\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size_tok = 2000\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=chunk_size_tok, chunk_overlap=0\n",
    ")\n",
    "texts_split = text_splitter.split_text(concatenated_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embd = OpenAIEmbeddings()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "RANDOM_SEED = 224  # Fixed seed for reproducibility\n",
    "\n",
    "### --- Code from citations referenced above (added comments and docstrings) --- ###\n",
    "\n",
    "\n",
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform global dimensionality reduction on the embeddings using UMAP.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - n_neighbors: Optional; the number of neighbors to consider for each point.\n",
    "                   If not provided, it defaults to the square root of the number of embeddings.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform local dimensionality reduction on the embeddings using UMAP, typically after global clustering.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - num_neighbors: The number of neighbors to consider for each point.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Determine the optimal number of clusters using the Bayesian Information Criterion (BIC) with a Gaussian Mixture Model.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - max_clusters: The maximum number of clusters to consider.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - An integer representing the optimal number of clusters found.\n",
    "    \"\"\"\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    n_clusters = np.arange(1, max_clusters)\n",
    "    bics = []\n",
    "    for n in n_clusters:\n",
    "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "    return n_clusters[np.argmin(bics)]\n",
    "\n",
    "\n",
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \"\"\"\n",
    "    Cluster embeddings using a Gaussian Mixture Model (GMM) based on a probability threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the cluster labels and the number of clusters determined.\n",
    "    \"\"\"\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    "\n",
    "\n",
    "def perform_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    threshold: float,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform clustering on the embeddings by first reducing their dimensionality globally, then clustering\n",
    "    using a Gaussian Mixture Model, and finally performing local clustering within each global cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for UMAP reduction.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster in GMM.\n",
    "\n",
    "    Returns:\n",
    "    - A list of numpy arrays, where each array contains the cluster IDs for each embedding.\n",
    "    \"\"\"\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        # Avoid clustering when there's insufficient data\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    # Global dimensionality reduction\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    # Global clustering\n",
    "    global_clusters, n_global_clusters = GMM_cluster(\n",
    "        reduced_embeddings_global, threshold\n",
    "    )\n",
    "\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    # Iterate through each global cluster to perform local clustering\n",
    "    for i in range(n_global_clusters):\n",
    "        # Extract embeddings belonging to the current global cluster\n",
    "        global_cluster_embeddings_ = embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    "\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            # Handle small clusters with direct assignment\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            # Local dimensionality reduction and clustering\n",
    "            reduced_embeddings_local = local_cluster_embeddings(\n",
    "                global_cluster_embeddings_, dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local, threshold\n",
    "            )\n",
    "\n",
    "        # Assign local cluster IDs, adjusting for total clusters already processed\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j + total_clusters\n",
    "                )\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters\n",
    "\n",
    "\n",
    "### --- Our code below --- ###\n",
    "\n",
    "\n",
    "def embed(texts):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of text documents.\n",
    "\n",
    "    This function assumes the existence of an `embd` object with a method `embed_documents`\n",
    "    that takes a list of texts and returns their embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be embedded.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: An array of embeddings for the given text documents.\n",
    "    \"\"\"\n",
    "    text_embeddings = embd.embed_documents(texts)\n",
    "    text_embeddings_np = np.array(text_embeddings)\n",
    "    return text_embeddings_np\n",
    "\n",
    "\n",
    "def embed_cluster_texts(texts):\n",
    "    \"\"\"\n",
    "    Embeds a list of texts and clusters them, returning a DataFrame with texts, their embeddings, and cluster labels.\n",
    "\n",
    "    This function combines embedding generation and clustering into a single step. It assumes the existence\n",
    "    of a previously defined `perform_clustering` function that performs clustering on the embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing the original texts, their embeddings, and the assigned cluster labels.\n",
    "    \"\"\"\n",
    "    text_embeddings_np = embed(texts)  # Generate embeddings\n",
    "    cluster_labels = perform_clustering(\n",
    "        text_embeddings_np, 10, 0.1\n",
    "    )  # Perform clustering on the embeddings\n",
    "    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n",
    "    df[\"text\"] = texts  # Store original texts\n",
    "    df[\"embd\"] = list(text_embeddings_np)  # Store embeddings as a list in the DataFrame\n",
    "    df[\"cluster\"] = cluster_labels  # Store cluster labels\n",
    "    return df\n",
    "\n",
    "\n",
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Formats the text documents in a DataFrame into a single string.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the 'text' column with text documents to format.\n",
    "\n",
    "    Returns:\n",
    "    - A single string where all text documents are joined by a specific delimiter.\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()\n",
    "    return \"--- --- \\n --- --- \".join(unique_txt)\n",
    "\n",
    "\n",
    "def embed_cluster_summarize_texts(\n",
    "    texts: List[str], level: int\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Embeds, clusters, and summarizes a list of texts. This function first generates embeddings for the texts,\n",
    "    clusters them based on similarity, expands the cluster assignments for easier processing, and then summarizes\n",
    "    the content within each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: A list of text documents to be processed.\n",
    "    - level: An integer parameter that could define the depth or detail of processing.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing two DataFrames:\n",
    "      1. The first DataFrame (`df_clusters`) includes the original texts, their embeddings, and cluster assignments.\n",
    "      2. The second DataFrame (`df_summary`) contains summaries for each cluster, the specified level of detail,\n",
    "         and the cluster identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed and cluster the texts, resulting in a DataFrame with 'text', 'embd', and 'cluster' columns\n",
    "    df_clusters = embed_cluster_texts(texts)\n",
    "\n",
    "    # Prepare to expand the DataFrame for easier manipulation of clusters\n",
    "    expanded_list = []\n",
    "\n",
    "    # Expand DataFrame entries to document-cluster pairings for straightforward processing\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "            )\n",
    "\n",
    "    # Create a new DataFrame from the expanded list\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "    # Retrieve unique cluster identifiers for processing\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    "\n",
    "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
    "\n",
    "    # Summarization\n",
    "    template = \"\"\"Here is a sub-set of LangChain Expression Language doc. \n",
    "    \n",
    "    LangChain Expression Language provides a way to compose chain in LangChain.\n",
    "    \n",
    "    Give a detailed summary of the documentation provided.\n",
    "    \n",
    "    Documentation:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "\n",
    "    # Format text within each cluster for summarization\n",
    "    summaries = []\n",
    "    for i in all_clusters:\n",
    "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt = fmt_txt(df_cluster)\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "\n",
    "    # Create a DataFrame to store summaries with their corresponding cluster and level\n",
    "    df_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"summaries\": summaries,\n",
    "            \"level\": [level] * len(summaries),\n",
    "            \"cluster\": list(all_clusters),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_clusters, df_summary\n",
    "\n",
    "\n",
    "def recursive_embed_cluster_summarize(\n",
    "    texts: List[str], level: int = 1, n_levels: int = 3\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Recursively embeds, clusters, and summarizes texts up to a specified level or until\n",
    "    the number of unique clusters becomes 1, storing the results at each level.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], texts to be processed.\n",
    "    - level: int, current recursion level (starts at 1).\n",
    "    - n_levels: int, maximum depth of recursion.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], a dictionary where keys are the recursion\n",
    "      levels and values are tuples containing the clusters DataFrame and summaries DataFrame at that level.\n",
    "    \"\"\"\n",
    "    results = {}  # Dictionary to store results at each level\n",
    "\n",
    "    # Perform embedding, clustering, and summarization for the current level\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    "\n",
    "    # Store the results of the current level\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # Determine if further recursion is possible and meaningful\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # Use summaries as the input texts for the next level of recursion\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    "\n",
    "        # Merge the results from the next level into the current results dictionary\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 1 clusters--\n"
     ]
    }
   ],
   "source": [
    "# Build tree\n",
    "leaf_texts = docs_texts\n",
    "results = recursive_embed_cluster_summarize(leaf_texts, level=1, n_levels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Initialize all_texts with leaf_texts\n",
    "all_texts = leaf_texts.copy()\n",
    "\n",
    "# Iterate through the results to extract summaries from each level and add them to all_texts\n",
    "for level in sorted(results.keys()):\n",
    "    # Extract summaries from the current level's DataFrame\n",
    "    summaries = results[level][1][\"summaries\"].tolist()\n",
    "    # Extend all_texts with the summaries from the current level\n",
    "    all_texts.extend(summaries)\n",
    "\n",
    "# Now, use all_texts to build the vectorstore with Chroma\n",
    "vectorstore = Chroma.from_texts(texts=all_texts, embedding=embd)\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A RAG chain in LangChain is defined using the LangChain Expression Language (LCEL), which allows for composing chains easily and efficiently. Here is a specific code example for defining a self-querying retriever in LangChain using LCEL: \\n\\n```python\\nfrom langchain.retrievers.self_query.base import SelfQueryRetriever\\nfrom langchain_openai import ChatOpenAI\\n\\nllm = ChatOpenAI(temperature=0)\\nretriever = SelfQueryRetriever.from_llm(\\n    llm,\\n    vectorstore,\\n    document_content_description,\\n    metadata_field_info,\\n)\\n```\\n\\nThis code snippet demonstrates how to create a self-querying retriever in LangChain using LCEL, enabling querying the retriever itself for structured information.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"How to define a RAG chain? Give me a specific code example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 14: ColBERT\n",
    "\n",
    "RAGatouille makes it as simple to use ColBERT.\n",
    "\n",
    "ColBERT generates a contextually influenced vector for each token in the passages.\n",
    "\n",
    "ColBERT similarly generates vectors for each token in the query.\n",
    "\n",
    "Then, the score of each document is the sum of the maximum similarity of each query embedding to any of the document embeddings:\n",
    "\n",
    "See here: \n",
    "- https://python.langchain.com/v0.2/docs/integrations/retrievers/ragatouille/\n",
    "- https://hackernoon.com/how-colbert-helps-developers-overcome-the-limits-of-rag\n",
    "- https://til.simonwillison.net/llms/colbert-ragatouille\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ragatouille\n",
      "  Downloading ragatouille-0.0.8.post2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting colbert-ai==0.2.19 (from ragatouille)\n",
      "  Downloading colbert-ai-0.2.19.tar.gz (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting faiss-cpu<2.0.0,>=1.7.4 (from ragatouille)\n",
      "  Downloading faiss_cpu-1.8.0.post1-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.7 kB)\n",
      "Collecting fast-pytorch-kmeans==0.2.0.1 (from ragatouille)\n",
      "  Downloading fast_pytorch_kmeans-0.2.0.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting langchain<0.2.0,>=0.1.0 (from ragatouille)\n",
      "  Using cached langchain-0.1.20-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langchain_core<0.2.0,>=0.1.4 (from ragatouille)\n",
      "  Using cached langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting llama-index>=0.7 (from ragatouille)\n",
      "  Downloading llama_index-0.10.50-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting onnx<2.0.0,>=1.15.0 (from ragatouille)\n",
      "  Downloading onnx-1.16.1-cp312-cp312-macosx_11_0_universal2.whl.metadata (16 kB)\n",
      "Collecting sentence-transformers<3.0.0,>=2.2.2 (from ragatouille)\n",
      "  Using cached sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting srsly==2.4.8 (from ragatouille)\n",
      "  Downloading srsly-2.4.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting torch>=1.13 (from ragatouille)\n",
      "  Downloading torch-2.3.1-cp312-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting transformers<5.0.0,>=4.36.2 (from ragatouille)\n",
      "  Using cached transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting voyager<3.0.0,>=2.0.2 (from ragatouille)\n",
      "  Downloading voyager-2.0.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.9 kB)\n",
      "Collecting bitarray (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading bitarray-2.9.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Collecting datasets (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting flask (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting git-python (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading git_python-1.0.3-py2.py3-none-any.whl.metadata (331 bytes)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (from colbert-ai==0.2.19->ragatouille) (1.0.1)\n",
      "Collecting ninja (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-macosx_10_9_universal2.macosx_10_9_x86_64.macosx_11_0_arm64.macosx_11_0_universal2.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (from colbert-ai==0.2.19->ragatouille) (1.14.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from colbert-ai==0.2.19->ragatouille) (4.66.4)\n",
      "Requirement already satisfied: ujson in ./.venv/lib/python3.12/site-packages (from colbert-ai==0.2.19->ragatouille) (5.10.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from fast-pytorch-kmeans==0.2.0.1->ragatouille) (1.26.4)\n",
      "Collecting pynvml (from fast-pytorch-kmeans==0.2.0.1->ragatouille)\n",
      "  Downloading pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.3 (from srsly==2.4.8->ragatouille)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from faiss-cpu<2.0.0,>=1.7.4->ragatouille) (24.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.12/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.12/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.12/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.venv/lib/python3.12/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.6.7)\n",
      "Collecting langchain-community<0.1,>=0.0.38 (from langchain<0.2.0,>=0.1.0->ragatouille)\n",
      "  Using cached langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain<0.2.0,>=0.1.0->ragatouille)\n",
      "  Using cached langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.venv/lib/python3.12/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.1.81)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.venv/lib/python3.12/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.7.4)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.12/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.venv/lib/python3.12/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (8.4.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.12/site-packages (from langchain_core<0.2.0,>=0.1.4->ragatouille) (1.33)\n",
      "Collecting packaging (from faiss-cpu<2.0.0,>=1.7.4->ragatouille)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_agent_openai-0.2.7-py3-none-any.whl.metadata (678 bytes)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_cli-0.1.12-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core==0.10.50 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_core-0.10.50-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl.metadata (604 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.2.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_llms_openai-0.1.23-py3-none-any.whl.metadata (610 bytes)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.1.6-py3-none-any.whl.metadata (677 bytes)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl.metadata (715 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_readers_file-0.1.25-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./.venv/lib/python3.12/site-packages (from llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (1.2.14)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core==0.10.50->llama-index>=0.7->ragatouille)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (2024.6.0)\n",
      "Requirement already satisfied: httpx in ./.venv/lib/python3.12/site-packages (from llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (0.27.0)\n",
      "Collecting llama-cloud<0.0.7,>=0.0.6 (from llama-index-core==0.10.50->llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_cloud-0.0.6-py3-none-any.whl.metadata (750 bytes)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in ./.venv/lib/python3.12/site-packages (from llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core==0.10.50->llama-index>=0.7->ragatouille)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting nltk<4.0.0,>=3.8.1 (from llama-index-core==0.10.50->llama-index>=0.7->ragatouille)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: openai>=1.1.0 in ./.venv/lib/python3.12/site-packages (from llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (1.35.3)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./.venv/lib/python3.12/site-packages (from llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (10.3.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in ./.venv/lib/python3.12/site-packages (from llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.12/site-packages (from llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./.venv/lib/python3.12/site-packages (from llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (0.9.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.12/site-packages (from llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in ./.venv/lib/python3.12/site-packages (from onnx<2.0.0,>=1.15.0->ragatouille) (4.25.3)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (1.5.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in ./.venv/lib/python3.12/site-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (0.23.4)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch>=1.13->ragatouille) (3.15.4)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.12/site-packages (from torch>=1.13->ragatouille) (1.12.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=1.13->ragatouille) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.19.1)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.36.2->ragatouille)\n",
      "  Downloading safetensors-0.4.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.0->ragatouille) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.0->ragatouille) (3.21.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain_core<0.2.0,>=0.1.4->ragatouille) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.0->ragatouille) (3.10.5)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in ./.venv/lib/python3.12/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille) (4.12.3)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille)\n",
      "  Using cached pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting llama-parse<0.5.0,>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index>=0.7->ragatouille)\n",
      "  Downloading llama_parse-0.4.4-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.0->ragatouille) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.0->ragatouille) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (2024.6.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->colbert-ai==0.2.19->ragatouille)\n",
      "  Using cached pyarrow-16.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->colbert-ai==0.2.19->ragatouille)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->colbert-ai==0.2.19->ragatouille)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets->colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama-index-core==0.10.50->llama-index>=0.7->ragatouille)\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting Werkzeug>=3.0.0 (from flask->colbert-ai==0.2.19->ragatouille)\n",
      "  Using cached werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting itsdangerous>=2.1.2 (from flask->colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: click>=8.1.3 in ./.venv/lib/python3.12/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (8.1.7)\n",
      "Collecting blinker>=1.6.2 (from flask->colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=1.13->ragatouille) (2.1.5)\n",
      "Collecting gitpython (from git-python->colbert-ai==0.2.19->ragatouille)\n",
      "  Using cached GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->ragatouille) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->ragatouille) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy->torch>=1.13->ragatouille) (1.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index>=0.7->ragatouille) (2.5)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx->llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx->llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (1.0.5)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from httpx->llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (0.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (1.9.0)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.50->llama-index>=0.7->ragatouille)\n",
      "  Downloading greenlet-3.0.3-cp312-cp312-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (1.0.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython->git-python->colbert-ai==0.2.19->ragatouille)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (2024.1)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai==0.2.19->ragatouille)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.50->llama-index>=0.7->ragatouille) (1.16.0)\n",
      "Downloading ragatouille-0.0.8.post2-py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fast_pytorch_kmeans-0.2.0.1-py3-none-any.whl (8.8 kB)\n",
      "Downloading srsly-2.4.8-cp312-cp312-macosx_11_0_arm64.whl (486 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.4/486.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.8.0.post1-cp312-cp312-macosx_11_0_arm64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
      "Downloading llama_index-0.10.50-py3-none-any.whl (6.8 kB)\n",
      "Downloading llama_index_core-0.10.50-py3-none-any.whl (15.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnx-1.16.1-cp312-cp312-macosx_11_0_universal2.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
      "Downloading torch-2.3.1-cp312-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "Downloading voyager-2.0.6-cp312-cp312-macosx_11_0_arm64.whl (352 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.6/352.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
      "Using cached langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
      "Downloading llama_index_agent_openai-0.2.7-py3-none-any.whl (12 kB)\n",
      "Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
      "Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl (6.2 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.2.1-py3-none-any.whl (9.1 kB)\n",
      "Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_llms_openai-0.1.23-py3-none-any.whl (11 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.1.6-py3-none-any.whl (5.8 kB)\n",
      "Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
      "Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Downloading llama_index_readers_file-0.1.25-py3-none-any.whl (37 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Downloading safetensors-0.4.3-cp312-cp312-macosx_11_0_arm64.whl (411 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading bitarray-2.9.2-cp312-cp312-macosx_11_0_arm64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.5/124.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading git_python-1.0.3-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading ninja-1.11.1.1-py2.py3-none-macosx_10_9_universal2.macosx_10_9_x86_64.macosx_11_0_arm64.macosx_11_0_universal2.whl (270 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.6/270.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading llama_cloud-0.0.6-py3-none-any.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_parse-0.4.4-py3-none-any.whl (8.0 kB)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyarrow-16.1.0-cp312-cp312-macosx_11_0_arm64.whl (26.0 MB)\n",
      "Using cached pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Using cached werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Downloading greenlet-3.0.3-cp312-cp312-macosx_11_0_universal2.whl (273 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.1/273.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: colbert-ai\n",
      "  Building wheel for colbert-ai (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for colbert-ai: filename=colbert_ai-0.2.19-py3-none-any.whl size=114760 sha256=b43a5d17c7bd230b8238cc8179306db6c8fb75880ef82c0c0e5f4d0b20cd0761\n",
      "  Stored in directory: /Users/jeongmin/Library/Caches/pip/wheels/25/7f/16/72ab365416054fcbbe579e61054f0c875e325d57c306f6dfbc\n",
      "Successfully built colbert-ai\n",
      "Installing collected packages: striprtf, ninja, dirtyjson, bitarray, xxhash, Werkzeug, voyager, smmap, safetensors, pypdf, pynvml, pyarrow-hotfix, pyarrow, packaging, onnx, nltk, networkx, itsdangerous, greenlet, fsspec, dill, catalogue, blinker, torch, srsly, multiprocess, gitdb, flask, faiss-cpu, llama-cloud, gitpython, fast-pytorch-kmeans, transformers, llama-index-legacy, llama-index-core, langchain_core, git-python, datasets, sentence-transformers, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, langchain-text-splitters, langchain-community, colbert-ai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, langchain, llama-index-program-openai, llama-index-question-gen-openai, llama-index, ragatouille\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.0\n",
      "    Uninstalling fsspec-2024.6.0:\n",
      "      Successfully uninstalled fsspec-2024.6.0\n",
      "  Attempting uninstall: langchain_core\n",
      "    Found existing installation: langchain-core 0.2.9\n",
      "    Uninstalling langchain-core-0.2.9:\n",
      "      Successfully uninstalled langchain-core-0.2.9\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.2.1\n",
      "    Uninstalling langchain-text-splitters-0.2.1:\n",
      "      Successfully uninstalled langchain-text-splitters-0.2.1\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.2.5\n",
      "    Uninstalling langchain-community-0.2.5:\n",
      "      Successfully uninstalled langchain-community-0.2.5\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.2.5\n",
      "    Uninstalling langchain-0.2.5:\n",
      "      Successfully uninstalled langchain-0.2.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-openai 0.1.9 requires langchain-core<0.3,>=0.2.2, but you have langchain-core 0.1.52 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Werkzeug-3.0.3 bitarray-2.9.2 blinker-1.8.2 catalogue-2.0.10 colbert-ai-0.2.19 datasets-2.20.0 dill-0.3.8 dirtyjson-1.0.8 faiss-cpu-1.8.0.post1 fast-pytorch-kmeans-0.2.0.1 flask-3.0.3 fsspec-2024.5.0 git-python-1.0.3 gitdb-4.0.11 gitpython-3.1.43 greenlet-3.0.3 itsdangerous-2.2.0 langchain-0.1.20 langchain-community-0.0.38 langchain-text-splitters-0.0.2 langchain_core-0.1.52 llama-cloud-0.0.6 llama-index-0.10.50 llama-index-agent-openai-0.2.7 llama-index-cli-0.1.12 llama-index-core-0.10.50 llama-index-embeddings-openai-0.1.10 llama-index-indices-managed-llama-cloud-0.2.1 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.23 llama-index-multi-modal-llms-openai-0.1.6 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.25 llama-index-readers-llama-parse-0.1.4 llama-parse-0.4.4 multiprocess-0.70.16 networkx-3.3 ninja-1.11.1.1 nltk-3.8.1 onnx-1.16.1 packaging-23.2 pyarrow-16.1.0 pyarrow-hotfix-0.6 pynvml-11.5.0 pypdf-4.2.0 ragatouille-0.0.8.post2 safetensors-0.4.3 sentence-transformers-2.7.0 smmap-5.0.1 srsly-2.4.8 striprtf-0.0.26 torch-2.3.1 transformers-4.41.2 voyager-2.0.6 xxhash-3.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ragatouille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongmin/PycharmProjects/rag-in-practice/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/jeongmin/PycharmProjects/rag-in-practice/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 26, 04:07:54] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongmin/PycharmProjects/rag-in-practice/.venv/lib/python3.12/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_wikipedia_page(title: str):\n",
    "    \"\"\"\n",
    "    Retrieve the full text content of a Wikipedia page.\n",
    "\n",
    "    :param title: str - Title of the Wikipedia page.\n",
    "    :return: str - Full text content of the page as raw string.\n",
    "    \"\"\"\n",
    "    # Wikipedia API endpoint\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "    }\n",
    "\n",
    "    # Custom User-Agent header to comply with Wikipedia's best practices\n",
    "    headers = {\"User-Agent\": \"RAGatouille_tutorial/0.0.1 (ben@clavie.eu)\"}\n",
    "\n",
    "    response = requests.get(URL, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extracting page content\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "    return page[\"extract\"] if \"extract\" in page else None\n",
    "\n",
    "full_document = get_wikipedia_page(\"Hayao_Miyazaki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
      "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
      "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
      "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
      "--------------------\n",
      "\n",
      "\n",
      "[Jun 26, 04:08:25] #> Creating directory .ragatouille/colbert/indexes/Miyazaki-123 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongmin/PycharmProjects/rag-in-practice/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 26, 04:08:28] [0] \t\t #> Encoding 78 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/Users/jeongmin/PycharmProjects/rag-in-practice/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 26, 04:08:31] [0] \t\t avg_doclen_est = 132.84616088867188 \t len(local_sample) = 78\n",
      "[Jun 26, 04:08:31] [0] \t\t Creating 1,024 partitions.\n",
      "[Jun 26, 04:08:31] [0] \t\t *Estimated* 10,362 embeddings.\n",
      "[Jun 26, 04:08:31] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/Miyazaki-123/plan.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: number of training points (9844) is less than the minimum recommended (10240)\n",
      "used 15 iterations (0.2328s) to cluster 9844 items into 1024 clusters\n",
      "[0.036, 0.04, 0.038, 0.036, 0.032, 0.037, 0.035, 0.038, 0.035, 0.036, 0.034, 0.036, 0.034, 0.037, 0.039, 0.037, 0.032, 0.032, 0.034, 0.039, 0.037, 0.038, 0.035, 0.036, 0.036, 0.034, 0.038, 0.036, 0.036, 0.036, 0.034, 0.036, 0.039, 0.034, 0.034, 0.033, 0.037, 0.034, 0.036, 0.038, 0.038, 0.038, 0.034, 0.032, 0.036, 0.032, 0.034, 0.033, 0.036, 0.034, 0.037, 0.034, 0.035, 0.037, 0.035, 0.036, 0.037, 0.038, 0.039, 0.032, 0.036, 0.037, 0.037, 0.034, 0.037, 0.036, 0.036, 0.036, 0.035, 0.033, 0.034, 0.033, 0.034, 0.038, 0.037, 0.033, 0.035, 0.037, 0.035, 0.038, 0.037, 0.04, 0.032, 0.042, 0.034, 0.035, 0.038, 0.039, 0.033, 0.042, 0.037, 0.037, 0.036, 0.036, 0.034, 0.034, 0.039, 0.035, 0.038, 0.036, 0.038, 0.04, 0.036, 0.033, 0.039, 0.035, 0.038, 0.034, 0.037, 0.034, 0.036, 0.037, 0.034, 0.033, 0.038, 0.035, 0.034, 0.033, 0.036, 0.038, 0.032, 0.033, 0.035, 0.036, 0.034, 0.036, 0.038, 0.037]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 26, 04:08:31] [0] \t\t #> Encoding 78 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.39it/s]\n",
      "1it [00:02,  2.19s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1807.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 26, 04:08:33] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Jun 26, 04:08:33] #> Building the emb2pid mapping..\n",
      "[Jun 26, 04:08:33] len(emb2pid) = 10362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1024/1024 [00:00<00:00, 171271.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 26, 04:08:33] #> Saved optimized IVF to .ragatouille/colbert/indexes/Miyazaki-123/ivf.pid.pt\n",
      "Done indexing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.ragatouille/colbert/indexes/Miyazaki-123'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAG.index(\n",
    "    collection=[full_document],\n",
    "    index_name=\"Miyazaki-123\",\n",
    "    max_document_length=180,\n",
    "    split_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading searcher for index Miyazaki-123 for the first time... This may take a few seconds\n",
      "[Jun 26, 04:08:48] #> Loading codec...\n",
      "[Jun 26, 04:08:48] #> Loading IVF...\n",
      "[Jun 26, 04:08:48] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jun 26, 04:08:55] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2128.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 26, 04:08:55] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 350.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 26, 04:08:55] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 26, 04:09:01] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "Searcher loaded!\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . What animation studio did Miyazaki found?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2054,  7284,  2996,  2106,  2771,  3148, 18637,  2179,\n",
      "         1029,   102,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103])\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongmin/PycharmProjects/rag-in-practice/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'content': 'Hayao Miyazaki (宮崎 駿 or 宮﨑 駿, Miyazaki Hayao, Japanese: [mijaꜜzaki hajao]; born January 5, 1941) is a Japanese animator, filmmaker, and manga artist. A founder of Studio Ghibli, he has attained international acclaim as a masterful storyteller and creator of Japanese animated feature films, and is widely regarded as one of the most accomplished filmmakers in the history of animation.\\nBorn in Tokyo City in the Empire of Japan, Miyazaki expressed interest in manga and animation from an early age, and he joined Toei Animation in 1963. During his early years at Toei Animation he worked as an in-between artist and later collaborated with director Isao Takahata.',\n",
       "  'score': 25.639408111572266,\n",
       "  'rank': 1,\n",
       "  'document_id': 'fdb0092b-20f8-4e78-868f-2aba5bea5585',\n",
       "  'passage_id': 0},\n",
       " {'content': \"In April 1984, Miyazaki opened his own office in Suginami Ward, naming it Nibariki.\\n\\n\\n=== Studio Ghibli ===\\n\\n\\n==== Early films (1985–1996) ====\\nOn June 15, 1985, Miyazaki and Takahata founded the animation production company Studio Ghibli as a subsidiary of Tokuma Shoten. Studio Ghibli's first film was Laputa: Castle in the Sky (1986), directed by Miyazaki. Some of the architecture in the film was also inspired by a Welsh mining town; Miyazaki witnessed the mining strike upon his first visit to Wales in 1984 and admired the miners' dedication to their work and community. Laputa was released on August 2, 1986, by the Toei Company.\",\n",
       "  'score': 25.61233901977539,\n",
       "  'rank': 2,\n",
       "  'document_id': 'fdb0092b-20f8-4e78-868f-2aba5bea5585',\n",
       "  'passage_id': 28},\n",
       " {'content': \"By this time, Miyazaki had moved to the animation studio Topcraft and was finding some of the staff to be unreliable. He eventually decided to bring on several of his previous collaborators for the film's production, including Takahata who would serve as producer. Pre-production began on May 31, 1983; Miyazaki encountered difficulties in creating the screenplay, with only sixteen chapters of the manga to work with. Takahata enlisted experimental and minimalist musician Joe Hisaishi to compose the film's score. Nausicaä of the Valley of the Wind was released on March 11, 1984. It grossed ¥1.48 billion at the box office, and made an additional ¥742 million in distribution income. It is often seen as Miyazaki's pivotal work, cementing his reputation as an animator.\",\n",
       "  'score': 25.073698043823242,\n",
       "  'rank': 3,\n",
       "  'document_id': 'fdb0092b-20f8-4e78-868f-2aba5bea5585',\n",
       "  'passage_id': 26}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = RAG.search(query=\"What animation studio did Miyazaki found?\", k=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongmin/PycharmProjects/rag-in-practice/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Hayao Miyazaki (宮崎 駿 or 宮﨑 駿, Miyazaki Hayao, Japanese: [mijaꜜzaki hajao]; born January 5, 1941) is a Japanese animator, filmmaker, and manga artist. A founder of Studio Ghibli, he has attained international acclaim as a masterful storyteller and creator of Japanese animated feature films, and is widely regarded as one of the most accomplished filmmakers in the history of animation.\\nBorn in Tokyo City in the Empire of Japan, Miyazaki expressed interest in manga and animation from an early age, and he joined Toei Animation in 1963. During his early years at Toei Animation he worked as an in-between artist and later collaborated with director Isao Takahata.'),\n",
       " Document(page_content=\"In April 1984, Miyazaki opened his own office in Suginami Ward, naming it Nibariki.\\n\\n\\n=== Studio Ghibli ===\\n\\n\\n==== Early films (1985–1996) ====\\nOn June 15, 1985, Miyazaki and Takahata founded the animation production company Studio Ghibli as a subsidiary of Tokuma Shoten. Studio Ghibli's first film was Laputa: Castle in the Sky (1986), directed by Miyazaki. Some of the architecture in the film was also inspired by a Welsh mining town; Miyazaki witnessed the mining strike upon his first visit to Wales in 1984 and admired the miners' dedication to their work and community. Laputa was released on August 2, 1986, by the Toei Company.\"),\n",
       " Document(page_content=\"By this time, Miyazaki had moved to the animation studio Topcraft and was finding some of the staff to be unreliable. He eventually decided to bring on several of his previous collaborators for the film's production, including Takahata who would serve as producer. Pre-production began on May 31, 1983; Miyazaki encountered difficulties in creating the screenplay, with only sixteen chapters of the manga to work with. Takahata enlisted experimental and minimalist musician Joe Hisaishi to compose the film's score. Nausicaä of the Valley of the Wind was released on March 11, 1984. It grossed ¥1.48 billion at the box office, and made an additional ¥742 million in distribution income. It is often seen as Miyazaki's pivotal work, cementing his reputation as an animator.\")]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = RAG.as_langchain_retriever(k=3)\n",
    "retriever.invoke(\"What animation studio did Miyazaki found?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
